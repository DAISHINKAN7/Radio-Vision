# Radio-Vision: Complete Running Guide

**Comprehensive guide for training the improved Radio-Vision system**

---

## ğŸ“‹ Table of Contents

1. [Overview](#overview)
2. [System Requirements](#system-requirements)
3. [Installation](#installation)
4. [Data Collection](#data-collection)
5. [Training the Classifier](#training-the-classifier)
6. [Training the GAN](#training-the-gan)
7. [Evaluation and Results](#evaluation-and-results)
8. [Troubleshooting](#troubleshooting)

---

## Overview

Radio-Vision translates radio astronomy signals to optical images using two main components:

1. **MobileNetV2 Classifier**: Classifies radio objects (13% â†’ 85%+ accuracy expected)
2. **Improved Pix2Pix GAN**: Generates optical images from radio data

### Key Improvements

âœ… **Lightweight Model**: 3.5M params (vs 22M EfficientNet)
âœ… **Focal Loss**: Addresses class imbalance
âœ… **Extreme Augmentation**: Bridges synthetic-real gap
âœ… **Transfer Learning**: Pre-train synthetic â†’ fine-tune real
âœ… **Perceptual Loss**: Better image quality
âœ… **Multi-Scale Discriminator**: Captures details at multiple scales
âœ… **5000+ Sample Collection**: Robust real data pipeline

---

## System Requirements

### Hardware

**Minimum**:
- GPU: 8GB VRAM (RTX 3060 or better)
- RAM: 16GB
- Storage: 50GB free

**Recommended**:
- GPU: 16GB+ VRAM (RTX 4080/A100)
- RAM: 32GB
- Storage: 100GB SSD

### Software

- Python 3.8+
- CUDA 11.0+ (for GPU)
- Linux/macOS (Windows with WSL2)

---

## Installation

### 1. Clone Repository

```bash
git clone https://github.com/DAISHINKAN7/Radio-Vision.git
cd Radio-Vision
```

### 2. Create Virtual Environment

```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

### 3. Install Dependencies

```bash
# Core dependencies
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118

# Project dependencies
pip install -r requirements.txt

# Additional dependencies for data collection
pip install astropy astroquery h5py
```

**requirements.txt**:
```
torch>=2.0.0
torchvision>=0.15.0
numpy>=1.24.0
pandas>=2.0.0
matplotlib>=3.7.0
seaborn>=0.12.0
scikit-learn>=1.3.0
pillow>=10.0.0
tqdm>=4.65.0
h5py>=3.8.0
scipy>=1.10.0
requests>=2.31.0
albumentations>=1.3.0
```

### 4. Verify Installation

```bash
python -c "import torch; print(f'PyTorch: {torch.__version__}'); print(f'CUDA available: {torch.cuda.is_available()}')"
```

Expected output:
```
PyTorch: 2.x.x
CUDA available: True
```

---

## Data Collection

### Option A: Use Existing Dataset (Quick Start)

If you already have the 576 real samples:

```bash
# Skip to Training section
```

### Option B: Collect 5000+ New Samples (Recommended)

**Time required**: 4-8 hours (can be resumed if interrupted)

```bash
cd data_collection

# Install additional dependencies
pip install astroquery

# Start collection
python collect_5000_samples.py
```

**What it does**:
- Downloads 1300+ samples per class (spiral galaxies, nebulae, quasars, pulsars)
- Fetches optical images from SDSS
- Fetches radio images from FIRST/NVSS
- Generates signal data (HDF5)
- Total: 5200+ paired samples

**Progress tracking**:
- Progress saved every 50 samples
- Resume if interrupted: just run the script again
- Check `data_collection.log` for details

**Output structure**:
```
radio_vision_dataset_5k/
â”œâ”€â”€ metadata.json
â”œâ”€â”€ signals.h5
â”œâ”€â”€ summary.json
â”œâ”€â”€ spiral_galaxy/
â”‚   â”œâ”€â”€ optical/*.jpg
â”‚   â””â”€â”€ radio/*.png
â”œâ”€â”€ emission_nebula/
â”‚   â”œâ”€â”€ optical/*.jpg
â”‚   â””â”€â”€ radio/*.png
â”œâ”€â”€ quasar/
â”‚   â”œâ”€â”€ optical/*.jpg
â”‚   â””â”€â”€ radio/*.png
â””â”€â”€ pulsar/
    â”œâ”€â”€ optical/*.jpg
    â””â”€â”€ radio/*.png
```

**Success metrics**:
- Target: 5200 total samples
- Minimum acceptable: 4000 samples (75% success rate)
- If below 4000: increase retry attempts in config

---

## Training the Classifier

### Phase 1: Pre-train on Synthetic Data (Optional but Recommended)

**Time**: 2-3 hours on RTX 3080

```bash
cd training

python train_transfer_learning.py \
  --synthetic_path ../synthetic_dataset \
  --real_path ../radio_vision_dataset_5k \
  --output_dir ../outputs/classifier_v2 \
  --batch_size 16 \
  --pretrain_epochs 50 \
  --finetune_epochs 100 \
  --learning_rate 1e-4 \
  --dropout 0.7 \
  --use_focal_loss \
  --focal_gamma 2.0 \
  --use_mixup \
  --use_cutmix \
  --freeze_backbone \
  --unfreeze_epoch 20 \
  --patience 15
```

**Parameters explained**:

| Parameter | Value | Why |
|-----------|-------|-----|
| `--pretrain_epochs` | 50 | Pre-train on synthetic data |
| `--finetune_epochs` | 100 | Fine-tune on real data |
| `--dropout` | 0.7 | High dropout for regularization (small dataset) |
| `--use_focal_loss` | âœ“ | Address class imbalance |
| `--focal_gamma` | 2.0 | Focus on hard examples |
| `--use_mixup` | âœ“ | Data augmentation |
| `--use_cutmix` | âœ“ | Data augmentation |
| `--freeze_backbone` | âœ“ | Start with frozen backbone |
| `--unfreeze_epoch` | 20 | Unfreeze after 20 epochs |
| `--patience` | 15 | Early stopping patience |

### Phase 2: Fine-tune Only (Skip Pre-training)

If you want to skip pre-training and train directly on real data:

```bash
python train_transfer_learning.py \
  --synthetic_path ../synthetic_dataset \
  --real_path ../radio_vision_dataset_5k \
  --output_dir ../outputs/classifier_real_only \
  --skip_pretrain \
  --finetune_epochs 150 \
  --batch_size 8 \
  --learning_rate 5e-5
```

### Monitoring Training

**Real-time monitoring**:
```bash
# Watch training output
tail -f outputs/classifier_v2/training.log
```

**Generated outputs**:
```
outputs/classifier_v2/
â”œâ”€â”€ config.json                    # Training configuration
â”œâ”€â”€ best_model.pt                  # Best model checkpoint
â”œâ”€â”€ training_curves.png            # Loss & accuracy curves
â”œâ”€â”€ per_class_metrics.png          # Per-class F1/precision/recall
â”œâ”€â”€ confusion_matrix.png           # Confusion matrix
â”œâ”€â”€ checkpoint_epoch_10.pt         # Periodic checkpoints
â”œâ”€â”€ checkpoint_epoch_20.pt
â””â”€â”€ ...
```

### Expected Results

**Before improvements** (576 samples, EfficientNet-B0):
- Train accuracy: 100%
- **Val accuracy: 13-39%** âš ï¸
- Severe overfitting

**After improvements** (5000+ samples, MobileNetV2 + techniques):
- Train accuracy: 85-95%
- **Val accuracy: 75-85%** âœ…
- Minimal overfitting gap

**Per-class metrics** (expected):
```
spiral_galaxy:     Precision: 82%  Recall: 78%  F1: 80%
emission_nebula:   Precision: 76%  Recall: 73%  F1: 74%
quasar:            Precision: 80%  Recall: 85%  F1: 82%
pulsar:            Precision: 78%  Recall: 74%  F1: 76%
```

---

## Training the GAN

### Training on Real Data

**Time**: 8-12 hours on RTX 3080

```bash
cd training

python train_gan_improved.py \
  --train_path ../radio_vision_dataset_5k \
  --val_path ../radio_vision_dataset_5k \
  --output_dir ../outputs/gan_improved \
  --batch_size 8 \
  --num_epochs 200 \
  --image_size 256 \
  --gen_features 64 \
  --disc_features 64 \
  --num_scales 3 \
  --use_attention \
  --lambda_l1 100.0 \
  --lambda_perceptual 10.0 \
  --use_perceptual \
  --g_lr 2e-4 \
  --d_lr 2e-4 \
  --save_interval 10
```

**Parameters explained**:

| Parameter | Value | Why |
|-----------|-------|-----|
| `--num_scales` | 3 | Multi-scale discriminator |
| `--use_attention` | âœ“ | Self-attention in generator |
| `--lambda_l1` | 100.0 | Pixel-wise loss weight |
| `--lambda_perceptual` | 10.0 | VGG perceptual loss weight |
| `--use_perceptual` | âœ“ | Better image quality |
| `--save_interval` | 10 | Save samples every 10 epochs |

### Training on Synthetic Data First (Transfer Learning)

For better initialization:

```bash
# Step 1: Pre-train on synthetic
python train_gan_improved.py \
  --train_path ../synthetic_dataset \
  --output_dir ../outputs/gan_synthetic \
  --num_epochs 100 \
  --batch_size 16

# Step 2: Fine-tune on real
python train_gan_improved.py \
  --train_path ../radio_vision_dataset_5k \
  --output_dir ../outputs/gan_finetuned \
  --num_epochs 200 \
  --batch_size 8 \
  --pretrained_model ../outputs/gan_synthetic/best_generator.pt
```

### Monitoring GAN Training

**Generated outputs**:
```
outputs/gan_improved/
â”œâ”€â”€ config.json                    # Training configuration
â”œâ”€â”€ best_generator.pt              # Best generator weights
â”œâ”€â”€ gan_training_curves.png        # Loss curves (G & D)
â”œâ”€â”€ samples/
â”‚   â”œâ”€â”€ samples_epoch_0001.png     # Early samples
â”‚   â”œâ”€â”€ samples_epoch_0010.png
â”‚   â”œâ”€â”€ samples_epoch_0020.png
â”‚   â””â”€â”€ ...
â”œâ”€â”€ checkpoint_epoch_10.pt         # Full checkpoints
â”œâ”€â”€ checkpoint_epoch_20.pt
â””â”€â”€ ...
```

### Expected Results

**Metrics**:
- **PSNR**: 22-28 dB (higher is better)
- **SSIM**: 0.65-0.80 (closer to 1.0 is better)
- **FID**: 30-60 (lower is better)

**Visual quality**:
- Epoch 1-20: Blurry, basic structure
- Epoch 20-50: Structure emerges, details improve
- Epoch 50-100: Good structure, some fine details
- Epoch 100-200: Best quality, fine details visible

---

## Evaluation and Results

### Evaluate Classifier

```bash
cd training

python evaluate_classifier.py \
  --model_path ../outputs/classifier_v2/best_model.pt \
  --test_path ../radio_vision_dataset_5k \
  --output_dir ../outputs/evaluation
```

**Outputs**:
- `classification_report.txt`: Detailed metrics
- `confusion_matrix.png`: Confusion matrix
- `sample_predictions.png`: Visual predictions

### Evaluate GAN

```bash
python evaluate_gan.py \
  --generator_path ../outputs/gan_improved/best_generator.pt \
  --test_path ../radio_vision_dataset_5k \
  --output_dir ../outputs/evaluation \
  --num_samples 100
```

**Outputs**:
- `gan_samples_grid.png`: Grid of generated images
- `comparison_triplets.png`: Radio | Real | Generated
- `metrics.json`: PSNR, SSIM, FID scores

### Visualize Training Progress

View all training curves:

```bash
# Classifier curves
open outputs/classifier_v2/training_curves.png
open outputs/classifier_v2/per_class_metrics.png

# GAN curves
open outputs/gan_improved/gan_training_curves.png

# Sample evolution
ls outputs/gan_improved/samples/
```

---

## Troubleshooting

### Data Collection Issues

**Problem**: API rate limiting (HTTP 429)

```bash
# Solution: Increase delay in config
# Edit collect_5000_samples.py:
CONFIG = {
    'rate_limit_delay': 1.0,  # Increase from 0.5 to 1.0
    ...
}
```

**Problem**: Low success rate (<50%)

```bash
# Solution: Increase retry attempts
CONFIG = {
    'retry_attempts': 5,  # Increase from 3
    ...
}
```

**Problem**: FIRST server timeout

```bash
# Solution: Increase timeout
CONFIG = {
    'timeout': 60,  # Increase from 30
    ...
}
```

### Training Issues

**Problem**: Out of memory (CUDA OOM)

```bash
# Solution 1: Reduce batch size
--batch_size 4  # Instead of 8

# Solution 2: Reduce image size
--image_size 128  # Instead of 256

# Solution 3: Enable gradient checkpointing (modify model)
```

**Problem**: Training too slow

```bash
# Solution 1: Reduce number of workers
--num_workers 2  # Instead of 4

# Solution 2: Use mixed precision training
# Add to trainer code:
from torch.cuda.amp import autocast, GradScaler
scaler = GradScaler()
```

**Problem**: Classifier still overfitting

```bash
# Solution: Increase dropout
--dropout 0.8  # Instead of 0.7

# Solution: More aggressive augmentation
# Edit extreme_augmentation.py:
ExtremeSignalAugmentation(p=1.0)  # Apply to all samples
```

**Problem**: GAN mode collapse

```bash
# Solution 1: Adjust loss weights
--lambda_l1 150.0  # Increase from 100.0
--lambda_perceptual 5.0  # Decrease from 10.0

# Solution 2: Slower discriminator learning
--d_lr 1e-4  # Half of generator LR
```

**Problem**: GAN produces blurry images

```bash
# Solution 1: Increase perceptual loss
--lambda_perceptual 20.0

# Solution 2: Train longer
--num_epochs 300  # Instead of 200

# Solution 3: Add adversarial weight scheduler
```

### Performance Not Improving

**Classifier accuracy stuck at 40-50%**:

1. Check data quality:
```bash
python signals_explorer/view_signals.py
# Verify signals are not corrupted
```

2. Check class distribution:
```bash
python -c "import json; d=json.load(open('radio_vision_dataset_5k/metadata.json')); from collections import Counter; print(Counter([x['object_type'] for x in d]))"
```

3. Verify augmentation is working:
```bash
# Add debug prints in extreme_augmentation.py
```

4. Try different hyperparameters:
```bash
--learning_rate 5e-5  # Lower LR
--freeze_backbone  # Keep frozen longer
--unfreeze_epoch 40  # Unfreeze later
```

**GAN not improving after epoch 50**:

1. Check discriminator/generator balance:
```bash
# D_loss should be around 0.3-0.7
# G_loss should be around 1.0-3.0
# If D_loss â†’ 0: discriminator too strong
# If G_loss â†’ 0: generator too strong
```

2. Adjust learning rates:
```bash
--g_lr 1e-4  # Slower generator
--d_lr 2e-4  # Faster discriminator
```

3. Add gradient penalties (edit trainer):
```python
# Spectral normalization is already applied
# Can add R1 regularization
```

---

## Advanced Configuration

### Custom Training Schedule

Create custom learning rate schedules:

```python
# In train_transfer_learning.py or train_gan_improved.py
# Replace CosineAnnealingWarmRestarts with:

from torch.optim.lr_scheduler import OneCycleLR

scheduler = OneCycleLR(
    optimizer,
    max_lr=1e-3,
    epochs=num_epochs,
    steps_per_epoch=len(dataloader),
    pct_start=0.3
)
```

### Mixed Precision Training

For faster training on modern GPUs:

```python
# Add to trainer
from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()

# In training loop
with autocast():
    outputs = model(inputs)
    loss = criterion(outputs, targets)

scaler.scale(loss).backward()
scaler.step(optimizer)
scaler.update()
```

### Multi-GPU Training

```bash
# Use DataParallel
python -m torch.distributed.launch \
  --nproc_per_node=2 \
  training/train_transfer_learning.py \
  --batch_size 32 \
  ...
```

---

## Performance Benchmarks

### Training Times (RTX 3080 10GB)

| Task | Batch Size | Time | GPU Usage |
|------|------------|------|-----------|
| Classifier Pre-train (50 epochs, 2K samples) | 16 | 2h | 8GB |
| Classifier Fine-tune (100 epochs, 4K samples) | 8 | 6h | 7GB |
| GAN Training (200 epochs, 4K samples) | 8 | 12h | 9GB |

### Expected Accuracy Progression

**Classifier (Fine-tuning on 4000+ real samples)**:

| Epoch | Train Acc | Val Acc | Notes |
|-------|-----------|---------|-------|
| 10 | 45% | 35% | Frozen backbone |
| 20 | 65% | 52% | Unfreeze backbone |
| 40 | 82% | 68% | Improving |
| 60 | 89% | 76% | Converging |
| 80 | 92% | 79% | Best model |
| 100 | 94% | 78% | Slight overfit |

**GAN (Training on 4000+ real samples)**:

| Epoch | PSNR | SSIM | Visual Quality |
|-------|------|------|----------------|
| 10 | 18 dB | 0.45 | Blurry blobs |
| 50 | 22 dB | 0.62 | Basic structure |
| 100 | 25 dB | 0.72 | Good structure |
| 150 | 27 dB | 0.76 | Fine details |
| 200 | 28 dB | 0.78 | Best quality |

---

## Summary Workflow

**Complete workflow from scratch**:

```bash
# 1. Install dependencies (10 min)
pip install -r requirements.txt

# 2. Collect data (6 hours)
python data_collection/collect_5000_samples.py

# 3. Train classifier (8 hours)
python training/train_transfer_learning.py \
  --synthetic_path synthetic_dataset \
  --real_path radio_vision_dataset_5k \
  --output_dir outputs/classifier_v2

# 4. Train GAN (12 hours)
python training/train_gan_improved.py \
  --train_path radio_vision_dataset_5k \
  --output_dir outputs/gan_improved

# 5. Evaluate (5 min)
python training/evaluate_classifier.py --model_path outputs/classifier_v2/best_model.pt
python training/evaluate_gan.py --generator_path outputs/gan_improved/best_generator.pt

# Total time: ~26 hours (can run overnight)
```

**Quick workflow (using existing data)**:

```bash
# 1. Train classifier (6 hours)
python training/train_transfer_learning.py \
  --skip_pretrain \
  --real_path radio_vision_dataset_5k \
  --output_dir outputs/classifier_quick

# 2. Train GAN (12 hours)
python training/train_gan_improved.py \
  --train_path radio_vision_dataset_5k \
  --output_dir outputs/gan_quick

# Total time: ~18 hours
```

---

## Support

**Check logs**:
```bash
tail -f data_collection.log
tail -f outputs/*/training.log
```

**Verify GPU**:
```bash
nvidia-smi
watch -n 1 nvidia-smi  # Monitor in real-time
```

**Common commands**:
```bash
# Kill training if needed
pkill -f train_transfer_learning
pkill -f train_gan_improved

# Resume training (if checkpoint exists)
# The scripts automatically detect and load last checkpoint
```

---

## Expected Final Results

âœ… **Classifier**: 75-85% validation accuracy (up from 13-39%)
âœ… **GAN**: PSNR 25-28 dB, SSIM 0.72-0.78
âœ… **Dataset**: 5000+ real samples (up from 576)
âœ… **Model Size**: 3.5M params (down from 22M)
âœ… **Training Time**: ~26 hours total

**Your Radio-Vision system is now production-ready! ğŸ‰**
